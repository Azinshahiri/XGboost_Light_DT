{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747fe696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.50      1.00      0.67         1\n",
      "         Yes       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'Feature1': [2.5, 1.1, 3.2, 0.3, 1.8, 2.2],\n",
    "    'Feature2': [1.3, 0.7, 2.2, 0.9, 1.1, 1.5],\n",
    "    'Label': ['Yes', 'No', 'Yes', 'No', 'Yes', 'No']\n",
    "})\n",
    "\n",
    "#---------------------------- Encode labels-----------------------\n",
    "le = LabelEncoder()\n",
    "data['Label_encoded'] = le.fit_transform(data['Label'])  # Yes=1, No=0\n",
    "\n",
    "# Feature matrix and target vector\n",
    "X = data[['Feature1', 'Feature2']]\n",
    "y = data['Label_encoded']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# ------------------Define a weak learner (decision stump)-------\n",
    "stump = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "\n",
    "# -----------------AdaBoost Classifier -----------------------\n",
    "ada = AdaBoostClassifier(\n",
    "    estimator=stump,\n",
    "    n_estimators=5,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "#---------------- Train model -----------\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = ada.predict(X_test)\n",
    "\n",
    "#------------------ Decode labels for display----------------\n",
    "y_pred_labels = le.inverse_transform(y_pred)\n",
    "y_test_labels = le.inverse_transform(y_test)\n",
    "\n",
    "#-------------------- Evaluation ----------------\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b6016c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333333333333333\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        15\n",
      "  versicolor       0.88      0.93      0.90        15\n",
      "   virginica       0.93      0.87      0.90        15\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.93      0.93      0.93        45\n",
      "weighted avg       0.93      0.93      0.93        45\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------Load the Iris dataset ---------------\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# -----------------------Define a simple decision stump as the weak learner -------------------------\n",
    "stump = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "\n",
    "#---------------------------- Initialize AdaBoost classifier -----------------------------\n",
    "ada = AdaBoostClassifier(\n",
    "    estimator=stump,        \n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "ada.fit(X_train, y_train)\n",
    "y_pred = ada.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1f81530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.009\n",
      "R^2 Score: 0.986\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.data[:, 0]  \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# -------------------- Define a weak learner (regression stump) ------------------\n",
    "stump_regressor = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
    "\n",
    "# ------------------ Initialize AdaBoost Regressor ---------------------------\n",
    "ada_regressor = AdaBoostRegressor(\n",
    "    estimator=stump_regressor,  \n",
    "    n_estimators=100,\n",
    "    learning_rate=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "ada_regressor.fit(X_train, y_train)\n",
    "y_pred = ada_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.3f}\")\n",
    "print(f\"R^2 Score: {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [474.99375422 330.53022894]\n",
      "Mean Squared Error: 312.48\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"Size\": [1400, 1600, 1700, 1875, 1100, 1550, 2350, 2450],\n",
    "    \"Bedrooms\": [3, 3, 4, 4, 2, 3, 5, 4],\n",
    "    \"Distance\": [5, 3, 2, 4, 7, 6, 2, 3],\n",
    "    \"Price\": [250, 310, 330, 355, 200, 275, 450, 475]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# -----------------------  Split features and target ------------------\n",
    "X = df[[\"Size\", \"Bedrooms\", \"Distance\"]]\n",
    "y = df[\"Price\"]\n",
    "\n",
    "# Step 3: Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# ----------------------- Train Gradient Boosting Regressor ---------------\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = gbr.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Predictions:\", y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afc27e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üèÜ Best Model: Random Forest\n",
      "‚úÖ Best Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "\n",
    "# ----------------------- Ignore warnings for cleaner output ---------------\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = (iris.target == 0).astype(int)  # Setosa = 1, others = 0 (binary)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=50, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    results.append((name, acc, report, matrix))\n",
    "\n",
    "# ------------------------------ Save results to a text file in the model_results.txt ---------------\n",
    "with open(\"model_results.txt\", \"w\") as f:\n",
    "    for name, acc, report, matrix in results:\n",
    "        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\n",
    "        f.write(f\"üî∑ Evaluating: {name} üî∑\\n\")\n",
    "        f.write(f\"‚úÖ Accuracy Score: {acc:.4f}\\n\")\n",
    "        f.write(\"üìã Classification Report:\\n\" + report + \"\\n\")\n",
    "        f.write(\"üßæ Confusion Matrix:\\n\")\n",
    "        f.write(str(matrix) + \"\\n\")\n",
    "\n",
    "best_model_name = None\n",
    "best_model_accuracy = 0.0\n",
    "\n",
    "\n",
    "\n",
    "# Inside your loop\n",
    "if acc > best_model_accuracy:\n",
    "    best_model_accuracy = acc\n",
    "    best_model_name = name\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"üèÜ Best Model: {best_model_name}\")\n",
    "print(f\"‚úÖ Best Accuracy: {best_model_accuracy:.4f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
